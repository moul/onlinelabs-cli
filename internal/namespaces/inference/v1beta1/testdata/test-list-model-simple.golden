🎲🎲🎲 EXIT CODE: 0 🎲🎲🎲
🟩🟩🟩 STDOUT️ 🟩🟩🟩️
ID                                    Name                                        Provider               Tags
6c68b256-ef4a-48d2-a794-9b63b5c6ba5d  meta/llama-3-8b-instruct:bf16               meta                   [instruct chat]
a7db3243-9d6a-4149-9ffb-081e24886d00  mistral/mixtral-8x7b-instruct-v0.1:fp16     mistral                [instruct chat]
c55d11d3-f717-4ede-82ac-48a74b84cbf1  mistral/mixtral-8x7b-instruct-v0.1:int8     mistral                [instruct chat]
3077ebd3-3a5a-47fc-91c2-da8cf6779c56  wizardlm/wizardlm-70b-v1.0:fp16             wizardlm               [instruct chat]
729c0386-ddde-4209-ba91-fad5ec47e2bb  wizardlm/wizardlm-70b-v1.0:fp8              wizardlm               [instruct]
4d834112-ed88-4394-9035-6eea562ec5f5  sentence-transformers/sentence-t5-xxl:fp32  sentence-transformers  [embedding]
12c8b72a-720e-4482-b9e1-527f7d8b7aea  meta/llama-3-70b-instruct:int8              meta                   [instruct chat]
🟩🟩🟩 JSON STDOUT 🟩🟩🟩
[
  {
    "id": "6c68b256-ef4a-48d2-a794-9b63b5c6ba5d",
    "name": "meta/llama-3-8b-instruct:bf16",
    "project_id": "",
    "provider": "meta",
    "tags": [
      "instruct",
      "chat"
    ],
    "description": "Efficient 8B-param model by Meta, fine-tuned for instruction and automation.",
    "has_eula": true,
    "created_at": "2024-03-14T00:00:00Z",
    "updated_at": "1970-01-01T00:00:00.0Z",
    "region": "fr-par",
    "is_public": true,
    "compatible_node_types": [
      "L4"
    ],
    "quantization_level": "bf16"
  },
  {
    "id": "a7db3243-9d6a-4149-9ffb-081e24886d00",
    "name": "mistral/mixtral-8x7b-instruct-v0.1:fp16",
    "project_id": "",
    "provider": "mistral",
    "tags": [
      "instruct",
      "chat"
    ],
    "description": "A high-quality Mixture of Experts (MoE) model with open weights by Mistral AI, licensed under Apache 2.0.",
    "has_eula": false,
    "created_at": "2024-03-15T00:00:00Z",
    "updated_at": "1970-01-01T00:00:00.0Z",
    "region": "fr-par",
    "is_public": true,
    "compatible_node_types": [
      "H100-2"
    ],
    "quantization_level": "fp16"
  },
  {
    "id": "c55d11d3-f717-4ede-82ac-48a74b84cbf1",
    "name": "mistral/mixtral-8x7b-instruct-v0.1:int8",
    "project_id": "",
    "provider": "mistral",
    "tags": [
      "instruct",
      "chat"
    ],
    "description": "A high-quality Mixture of Experts (MoE) model with open weights by Mistral AI, licensed under Apache 2.0.",
    "has_eula": false,
    "created_at": "2024-03-15T00:00:00Z",
    "updated_at": "1970-01-01T00:00:00.0Z",
    "region": "fr-par",
    "is_public": true,
    "compatible_node_types": [
      "H100"
    ],
    "quantization_level": "int8"
  },
  {
    "id": "3077ebd3-3a5a-47fc-91c2-da8cf6779c56",
    "name": "wizardlm/wizardlm-70b-v1.0:fp16",
    "project_id": "",
    "provider": "wizardlm",
    "tags": [
      "instruct",
      "chat"
    ],
    "description": "General use 70B-param model based on Llama 2.",
    "has_eula": true,
    "created_at": "2024-03-15T12:00:00Z",
    "updated_at": "1970-01-01T00:00:00.0Z",
    "region": "fr-par",
    "is_public": true,
    "compatible_node_types": [
      "H100-2"
    ],
    "quantization_level": "fp16"
  },
  {
    "id": "729c0386-ddde-4209-ba91-fad5ec47e2bb",
    "name": "wizardlm/wizardlm-70b-v1.0:fp8",
    "project_id": "",
    "provider": "wizardlm",
    "tags": [
      "instruct"
    ],
    "description": "Empowering Large Pre-Trained Language Models to Follow Complex Instructions",
    "has_eula": true,
    "created_at": "2024-03-15T12:00:00Z",
    "updated_at": null,
    "region": "fr-par",
    "is_public": true,
    "compatible_node_types": [
      "H100"
    ],
    "quantization_level": "fp8"
  },
  {
    "id": "4d834112-ed88-4394-9035-6eea562ec5f5",
    "name": "sentence-transformers/sentence-t5-xxl:fp32",
    "project_id": "",
    "provider": "sentence-transformers",
    "tags": [
      "embedding"
    ],
    "description": "Model from pre-trained Text-to-Text sentence encode.",
    "has_eula": false,
    "created_at": "1970-01-01T00:00:00.0Z",
    "updated_at": "1970-01-01T00:00:00.0Z",
    "region": "fr-par",
    "is_public": true,
    "compatible_node_types": [
      "L4"
    ],
    "quantization_level": "fp32"
  },
  {
    "id": "12c8b72a-720e-4482-b9e1-527f7d8b7aea",
    "name": "meta/llama-3-70b-instruct:int8",
    "project_id": "",
    "provider": "meta",
    "tags": [
      "instruct",
      "chat"
    ],
    "description": "Latest 70B-param model from Meta, fine-tuned for instruction and automation.",
    "has_eula": true,
    "created_at": "1970-01-01T00:00:00.0Z",
    "updated_at": "1970-01-01T00:00:00.0Z",
    "region": "fr-par",
    "is_public": true,
    "compatible_node_types": [
      "H100"
    ],
    "quantization_level": "int8"
  }
]
